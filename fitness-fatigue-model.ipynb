{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitness - fatigue modelling with Banister\n",
    "====\n",
    "\n",
    "This notebook deals with the Banister models \\cite{banister1975systems, busso1994} on the evaluation and prediction of athlete performance in function of fatigue and fitness. The general equation is given below:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t}=p_{0}+k_{a} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{a}} w_{s}-k_{f} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{f}} w_s\n",
    "\\label{eq:banister}\n",
    "\\end{equation}\n",
    "\n",
    "* $p_{t}$ is the modelled performance at time $t$\n",
    "* $p_0$ is the initial performance level\n",
    "* $k_a$ and $k_f$ are fitness and fatigue magnitude factor\n",
    "* $\\tau_a$ and $\\tau_f$ are fitness and fatigue time decay\n",
    "* $w_s$ is the known training load per time unit, or ' the known training load per week (or day) from the first week of training to the week (or day) preceding the performance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are however some ***issues with the Banister model*** which are described in detail in \\cite{Hellard2006}.\n",
    "1. The model parameters suffer from poor identifiability, which seems logical as the mechanisms could correct for eachother. This feature leads to a lot of variability in the resulting parameter estimates. The model somehow has been reported to predict reasonably well. The model does have 4 parameters that can bring this in. \n",
    "2. On top of that there is usually only limited data, between 10 and 20 data points per athlete, to fit the model.\n",
    "3. The Banister model parameters might not be so physically accurate, the model plausibility could be low.\n",
    "\n",
    "There appears to be the upside that the model is stable and parameter estimates do not change much (on top of the other variability?) when a data point is left out.\n",
    "\n",
    "\n",
    "Things to check on \\cite{Hellard2006} still:\n",
    "1. how was the model prediction assessed?\n",
    "2. how was bootstrapping used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:23:55.603877Z",
     "start_time": "2020-05-05T07:23:49.098505Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from palettable.scientific import sequential\n",
    "# calculations\n",
    "from scipy.optimize import brute, differential_evolution\n",
    "# sensitivity analysis\n",
    "from SALib.analyze import sobol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming of parameters in plots\n",
    "parname_dict = {\n",
    "    'k_a' : '$k_a$',\n",
    "    'k_f' : '$k_f$',\n",
    "    'tau_a' : r'$\\tau_a$',\n",
    "    'tau_f' : r'$\\tau_f$',\n",
    "    'p_0' : '$p_0$',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall plot settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# get all sporter dataset names\n",
    "dataset_names = [f for f in os.listdir() if 'modellen.xlsx' in f]\n",
    "dataset_dict = dict() #dictionary to store all data in\n",
    "for d in dataset_names:\n",
    "    # read sporter excel file and store in dictionary, referenced by sporter name\n",
    "    dataset_dict[d[:-14]] = pd.read_excel(d, sheet_name='TSS', index_col=0).loc[:,['Datum', 'TL', 'TT (W)', 'PTE', 'Fitness', 'NTE', 'Fatigue',\n",
    "       'TT performance', 'Performance', 'Modelfit']] #only store columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#create figure\n",
    "fig, ax  = plt.subplots(figsize=(12,3))\n",
    "#select dataset\n",
    "sporter = 'Wietse'\n",
    "data2plot = dataset_dict[sporter]\n",
    "#plot data\n",
    "ax.plot(data2plot.loc[:,'TT (W)'],'*', label='Data '+sporter)\n",
    "#format the figure\n",
    "ax.set_xlim(0,100);\n",
    "ax.set_xlabel('Time, in days');\n",
    "ax.set_ylabel('Performance,\\nin arbitrary units');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banister model\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t}=p_{0}+k_{a} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{a}} w_{s}-k_{f} \\sum_{s=0}^{t-1} e^{-(t-s) / \\tau_{f}} w_s\n",
    "\\label{eq:banister2}\n",
    "\\end{equation}\n",
    "\n",
    "Below is the implementation for Eq.\\ref{eq:banister}. *The same notation is used, thus the meaning of the inputs to the model function* `ff_banister` *can be found under Eq.\\ref{eq:banister}*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def sum_exp(t, k, tau, w_s):\n",
    "    \"\"\"calculation for summation exponential term (is this a convolution?)\n",
    "    needed for the calculation of the fitness and fatigue terms\"\"\"\n",
    "    retval = np.zeros((len(t),1))\n",
    "    for i in range(1,len(t)-1):\n",
    "        retval[i] = np.sum(np.exp(-(i-np.arange(i+1))/tau)*w_s[:i+1])\n",
    "        \n",
    "    return k*retval\n",
    "\n",
    "def ff_banister(t, p_0, k_a, k_f, tau_a, tau_f, w_t, tt_W):\n",
    "    \"\"\"\n",
    "    returns the performance at time t calculated with all the input parameters\n",
    "    \"\"\"\n",
    "    p_t = p_0 + sum_exp(t, k_a, tau_a, w_t) - sum_exp(t, k_f, tau_f, w_t)\n",
    "    \n",
    "    for ti, tv in enumerate(tt_W):\n",
    "        if tv>0.1:\n",
    "            p_t[ti] = p_t[ti] + 2*(k_f-k_a)*w_t[ti-1]\n",
    "            print(ti)\n",
    "    \n",
    "    return p_t\n",
    "\n",
    "def kobes_way(t, p_0, k_a, k_f, tau_a, tau_f, w_t, tt_W):\n",
    "    \"\"\"\n",
    "    try exactly the  code from the excel file\n",
    "    \"\"\"\n",
    "    retval = np.zeros((len(t),1))\n",
    "    retval[0] = p_0\n",
    "    pte = np.zeros((len(t),1))\n",
    "    nte = np.zeros((len(t),1))\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        pte[i] = (pte[i-1]*np.exp(-1/tau_a) + w_t[i])\n",
    "        nte[i] = (nte[i-1]*np.exp(-1/tau_f) + w_t[i])\n",
    "        if tt_W[i]<0.1:\n",
    "            retval[i] = p_0 + (pte[i])*k_a - (nte[i])*k_f\n",
    "        else:\n",
    "            retval[i] = p_0 + (pte[i-1]*np.exp(-1/tau_a))*k_a - (nte[i-1]*np.exp(-1/tau_f))*k_f\n",
    "            \n",
    "    simres = np.array((retval, pte, nte)).reshape(3, len(t)).transpose()\n",
    "        \n",
    "    return pd.DataFrame(simres, index=t, columns=['overall', 'fit', 'fat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data\n",
    "\n",
    "Select an athlete out of Thibaux, Wietse or Tom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inputs\n",
    "sporter = 'Thibaux'\n",
    "data4mod = dataset_dict[sporter]\n",
    "t = data4mod.index.values\n",
    "data4mod[['TT (W)']] = data4mod[['TT (W)']].fillna(value=0)\n",
    "data4mod[['TL']] = data4mod[['TL']].fillna(value=0)\n",
    "w_t = data4mod[['TL']].values.ravel()\n",
    "tt_w = data4mod[['TT (W)']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying model implementation\n",
    "\n",
    "Checking whether simulated outcomes are the same as those of Kobe V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using parameters from the excel and compare with Kobe's dank excel code\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "mod_out = kobes_way(t, 240, 0.592627747, 0.830655943, 11.23205693, 6.327383027, w_t, tt_w)\n",
    "plt.plot(t, data4mod.loc[:,'Modelfit'], label='Kobe')\n",
    "plt.plot(mod_out['overall'] , '--', label='Michael')\n",
    "# plt.plot(t, data4mod.loc[:,'TL'], '*', label='TL')\n",
    "#plot data\n",
    "ax.plot(t[tt_w>0.1], tt_w[tt_w>0.1],'*', label='Data '+sporter)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "Visualise the objective function \n",
    "Conduct a global optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "def mod_err_banister(pars, t_loc, w_t_loc, tt_w_loc, err_type='sse', p_0=240):\n",
    "    \"\"\"\n",
    "    pars {list}: contains the parameter values of the Banister model in the following order:\n",
    "        [k_a, k_f, tau_a, tau_f]\n",
    "    t_loc {np.array 1D}: time denotation (in days)\n",
    "    w_t_loc {np.array 1D}: values from TL column in data\n",
    "    tt_w_loc {np.array 1D}: values from TT (W) column in data\n",
    "    err_type {string}: specifies model error calculation type, standard sum of squared errors\n",
    "    \"\"\"\n",
    "    # parameter values\n",
    "    [k_a, k_f, tau_a, tau_f] = pars\n",
    "    # calculate model output\n",
    "    mod_out = kobes_way(t_loc, p_0, k_a, k_f, tau_a, tau_f, w_t_loc, tt_w_loc)\n",
    "    # reformat data for model error calculation\n",
    "    data2compare = tt_w_loc.copy()\n",
    "    data2compare[tt_w_loc<0.1] = np.nan\n",
    "    # model error calculation\n",
    "    if err_type == 'sse': #sum of squared errors\n",
    "        mod_err = np.nansum(np.square(mod_out.overall.values - data2compare))\n",
    "        \n",
    "    return mod_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual optimisation\n",
    "\n",
    "A brute force is carried out, which checks the model fit at a range of specified parameter value combinations.\n",
    "This is mainly to visualise the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "par_ranges = (slice(0, 2, 0.1), slice(0, 2, 0.1),\n",
    "             slice(3, 30, 1), slice(3, 30,1))\n",
    "\n",
    "par_ranges_dict = {\n",
    "    'k_a' : np.arange(0,2,step=0.1),\n",
    "    'k_f' : np.arange(0,2,step=0.1),\n",
    "    'tau_a' : np.arange(3,20,step=1),\n",
    "    'tau_f' : np.arange(3,20,step=1),\n",
    "}\n",
    "results = brute(mod_err_banister, par_ranges, args=(t, w_t, tt_w), full_output=True, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xeval = results[2]\n",
    "feval = results[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# how to plot this\n",
    "\n",
    "fig_brutres = plt.figure(figsize=(10,7), constrained_layout=True)\n",
    "kleurmap = cm.viridis_r #sequential.Bilbao_10.mpl_colormap\n",
    "cont_nlev = 100\n",
    "\n",
    "spec = gridspec.GridSpec(ncols=41, nrows=2, figure=fig_brutres)\n",
    "ax11 = fig_brutres.add_subplot(spec[0, :20])\n",
    "ax12 = fig_brutres.add_subplot(spec[0, 20:-1])\n",
    "ax21 = fig_brutres.add_subplot(spec[1, :20])\n",
    "ax22 = fig_brutres.add_subplot(spec[1, 20:-1])\n",
    "ax_cb = fig_brutres.add_subplot(spec[:,-1])\n",
    "# we plot each optimal objective function value for a combination of two selected parameters\n",
    "# normalising to the same colour scale\n",
    "normF = plt.Normalize(20, 100)\n",
    "\n",
    "# plot optimal value found in function of combination of values of two parameters\n",
    "# right now I have no idea but to rewrite this manually\n",
    "# bc changing the parameter selection also changes the indexing\n",
    "\n",
    "# k_a vs k_f\n",
    "dims = [0,1]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,j,:,:]))\n",
    "\n",
    "# plot contour\n",
    "mappab = ax11.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax11.set_xlabel(parname_dict[parnames[0]])\n",
    "ax11.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# tau\n",
    "dims = [2,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,:,i,j]))\n",
    "\n",
    "# plot contour\n",
    "ax12.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax12.set_xlabel(parname_dict[parnames[0]])\n",
    "ax12.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# fitness\n",
    "dims = [0,2]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,:,j,:]))\n",
    "\n",
    "# plot contour\n",
    "ax21.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax21.set_xlabel(parname_dict[parnames[0]])\n",
    "ax21.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# fatigue\n",
    "dims = [1,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,i,:,j]))\n",
    "\n",
    "# plot contour\n",
    "ax22.contourf(P1, P2, F, levels=cont_nlev, norm=normF, cmap=kleurmap)\n",
    "ax22.set_xlabel(parname_dict[parnames[0]])\n",
    "ax22.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# colorbar\n",
    "cb = fig_brutres.colorbar(mappab, cax=ax_cb)\n",
    "cb.set_label('absolute model error', fontsize=16)\n",
    "\n",
    "fig_brutres.savefig(sporter+'_brutres.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results\n",
    "# optim_parval = pd.DataFrame(index=['Thibaux', 'Wietse', 'Tom'], columns = ['k_a', 'k_f', 'tau_a', 'tau_f'])\n",
    "optim_parval.loc[sporter,:] = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and stop model\n",
    "\n",
    "Or 'prikkel' and 'stop' model: calculation of the ideal times to respectively have the last big exercise and when to stop exercising prior to a contest (when the athlete should peak). Need to check this\n",
    "\n",
    "\n",
    "\n",
    "This is calculated based on the fitter Banister model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "def spark_stop(pars):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - pars {list}: contains the parameter values of the Banister model in the following order:\n",
    "        [k_a, k_f, tau_a, tau_f]\n",
    "    \"\"\"\n",
    "    # parameter values\n",
    "    [k_a, k_f, tau_a, tau_f] = pars    \n",
    "    spark = (tau_a*tau_f/(tau_a - tau_f)) * np.log((k_f*tau_a)/(k_a*tau_f))\n",
    "    stop = (tau_a*tau_f/(tau_a - tau_f)) * np.log(k_f/k_a)\n",
    "    return spark, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_stop([0.634,0.716,9.541,7.812])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol, delta\n",
    "from SALib.sample import saltelli, latin\n",
    "from SALib.plotting import bar, morris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model inputs\n",
    "problem = {\n",
    "    'num_vars': 4,\n",
    "    'names': ['k_a', 'k_f', 'tau_a', 'tau_f'],\n",
    "    'bounds': [[0, 3],[0, 3],[0, 60],[0, 60]]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = latin.sample(problem, 10000)\n",
    "\n",
    "# Run model (example)\n",
    "Y = np.array([spark_stop(p) for p in param_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis\n",
    "Y_in = Y[:,1].copy()\n",
    "good_values = ~((sobol_in<0) | (np.isinf(sobol_in)) | (np.isnan(sobol_in)))\n",
    "Y_in = Y_in[good_values] \n",
    "X_in = param_values[good_values,:]\n",
    "Si = delta.analyze(problem, X_in, Y_in, print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "Visualise objective function and conduct a global optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "def mod_err_banister(pars, t_loc, w_t_loc, tt_w_loc, err_type='sse', p_0=240):\n",
    "    \"\"\"\n",
    "    pars {list}: contains the parameter values of the Banister model in the following order:\n",
    "        [k_a, k_f, tau_a, tau_f]\n",
    "    t_loc {np.array 1D}: time denotation (in days)\n",
    "    w_t_loc {np.array 1D}: values from TL column in data\n",
    "    tt_w_loc {np.array 1D}: values from TT (W) column in data\n",
    "    err_type {string}: specifies model error calculation type, standard sum of squared errors\n",
    "    \"\"\"\n",
    "    # parameter values\n",
    "    [k_a, k_f, tau_a, tau_f] = pars\n",
    "    # calculate model output\n",
    "    mod_out = kobes_way(t_loc, p_0, k_a, k_f, tau_a, tau_f, w_t_loc, tt_w_loc)\n",
    "    # reformat data for model error calculation\n",
    "    data2compare = tt_w_loc.copy()\n",
    "    data2compare[tt_w_loc<0.1] = np.nan\n",
    "    # model error calculation\n",
    "    if err_type == 'sse': #sum of squared errors\n",
    "        mod_err = np.nansum(np.square(mod_out.overall.values - data2compare))\n",
    "        \n",
    "    return mod_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brute force is carried out, which checks the model fit at a range of specified parameter value combinations. This is mainly to visualise the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter ranges \n",
    "par_ranges = (slice(0, 3, 0.1), slice(0, 3, 0.1),\n",
    "             slice(0, 60, 1), slice(0,60,1))\n",
    "\n",
    "par_ranges_dict = {\n",
    "    'k_a' : np.arange(0,3,step=0.1),\n",
    "    'k_f' : np.arange(0,3,step=0.1),\n",
    "    'tau_a' : np.arange(0,60,step=1),\n",
    "    'tau_f' : np.arange(0,60,step=1),\n",
    "}\n",
    "\n",
    "lowbounds = [l[0] for l in list(par_ranges_dict.values())]\n",
    "uppbounds = [l[-1] for l in list(par_ranges_dict.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = brute(mod_err_banister, par_ranges, args=(t, w_t, tt_w), full_output=True, disp=True, workers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig_brutres = plt.figure(figsize=(10,7), constrained_layout=True)\n",
    "kleurmap = cm.viridis_r #sequential.Bilbao_10.mpl_colormap\n",
    "\n",
    "spec = gridspec.GridSpec(ncols=41, nrows=2, figure=fig_brutres)\n",
    "ax11 = fig_brutres.add_subplot(spec[0, :20])\n",
    "ax12 = fig_brutres.add_subplot(spec[0, 20:-1])\n",
    "ax21 = fig_brutres.add_subplot(spec[1, :20])\n",
    "ax22 = fig_brutres.add_subplot(spec[1, 20:-1])\n",
    "ax_cb = fig_brutres.add_subplot(spec[:,-1])\n",
    "# we plot each optimal objective function value for a combination of two selected parameters\n",
    "# normalising to the same colour scale\n",
    "normF = plt.Normalize(0, 200)\n",
    "\n",
    "# plot optimal value found in function of combination of values of two parameters\n",
    "# right now I have no idea but to rewrite this manually\n",
    "# bc changing the parameter selection also changes the indexing\n",
    "\n",
    "# k_a vs k_f\n",
    "dims = [0,1]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,j,:,:]))\n",
    "\n",
    "# plot contour\n",
    "mappab = ax11.contourf(P1, P2, F, norm=normF, cmap=kleurmap)\n",
    "ax11.set_xlabel(parname_dict[parnames[0]])\n",
    "ax11.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# tau\n",
    "dims = [2,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,:,i,j]))\n",
    "\n",
    "# plot contour\n",
    "ax12.contourf(P1, P2, F, norm=normF, cmap=kleurmap)\n",
    "ax12.set_xlabel(parname_dict[parnames[0]])\n",
    "ax12.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# fitness\n",
    "dims = [0,2]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[i,:,j,:]))\n",
    "\n",
    "# plot contour\n",
    "ax21.contourf(P1, P2, F, norm=normF, cmap=kleurmap)\n",
    "ax21.set_xlabel(parname_dict[parnames[0]])\n",
    "ax21.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "# fatigue\n",
    "dims = [1,3]\n",
    "parnames = [list(par_ranges_dict.keys())[i] for i in dims]\n",
    "disc1 = np.shape(feval)[dims[0]]\n",
    "disc2 = np.shape(feval)[dims[1]]\n",
    "# storage of parameter value combinations\n",
    "p1 = par_ranges_dict[parnames[0]]\n",
    "p2 = par_ranges_dict[parnames[1]]\n",
    "P1, P2 = np.meshgrid(p1, p2)\n",
    "F = P1*2 +P2\n",
    "\n",
    "#loop over first variable\n",
    "for i in range(disc1):\n",
    "    #loop over second variable\n",
    "    for j in range(disc2):\n",
    "#         print(j)\n",
    "        F[j,i] = np.sqrt(np.min(feval[:,i,:,j]))\n",
    "\n",
    "# plot contour\n",
    "ax22.contourf(P1, P2, F, norm=normF, cmap=kleurmap)\n",
    "ax22.set_xlabel(parname_dict[parnames[0]])\n",
    "ax22.set_ylabel(parname_dict[parnames[1]]);\n",
    "\n",
    "\n",
    "# colorbar\n",
    "cb = fig_brutres.colorbar(mappab, cax=ax_cb)\n",
    "cb.set_label('absolute model error', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global optimisation for every exercise data collection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particle swarm optimisation\n",
    "The [particle swarm optimisation (PSO) technique](https://en.wikipedia.org/wiki/Particle_swarm_optimization) is a strategy for global optimisation. It is easy to conduct in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_optires_pso = dict()\n",
    "n_opts=3\n",
    "for sporter in list(dataset_dict.keys()):\n",
    "    dict_optires_pso[sporter]=dict()\n",
    "    for s in sheetnames:\n",
    "        data4mod = dataset_dict[sporter][s]\n",
    "        t = data4mod.index.values\n",
    "        data4mod[['TT (W)']] = data4mod[['TT (W)']].fillna(value=0)\n",
    "        data4mod[['TL']] = data4mod[['TL']].fillna(value=0)\n",
    "        w_t = data4mod[['TL']].values.ravel()\n",
    "        tt_w = data4mod[['TT (W)']].values.ravel()\n",
    "        dict_optires_pso[sporter][s] = pd.DataFrame(index = np.arange(n_opts), columns=list(par_ranges_dict.keys())+['f_opt'])\n",
    "        \n",
    "        # conduct a few optimisations\n",
    "        for o in range(n_opts):\n",
    "            pso_x, pso_f = pso(mod_err_banister, lowbounds, uppbounds, ieqcons=[], f_ieqcons=None, args=(t, w_t, tt_w), kwargs={},\n",
    "                   swarmsize=1280, omega=0.7, phip=0.2, phig=0.7, maxiter=1000, minstep=1e-8, minfunc=1e-8, debug=True, processes=16)\n",
    "            dict_optires_pso[sporter][s].iloc[o,:-1] = pso_x\n",
    "            dict_optires_pso[sporter][s].iloc[o,-1] = pso_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format results as a pandas dataframe\n",
    "mind_optires_pso = pd.MultiIndex.from_product([list(dict_optires_pso.keys()), sheetnames, np.arange(n_opts)])\n",
    "df_optires_pso = pd.DataFrame(index=mind_optires_pso, columns=list(par_ranges_dict.keys())+['f_opt'])\n",
    "\n",
    "# get all optimisation results from the relevant dictionary\n",
    "for sporter in list(dataset_dict.keys()):\n",
    "    for s in sheetnames:\n",
    "        df_optires_pso.loc[(sporter, s),:] = dict_optires_pso[sporter][s].values\n",
    "\n",
    "# calculate absolute error, assuming model objective function was SSE\n",
    "df_optires_pso.loc[:,'abs_err'] = (df_optires_pso.loc[:,'f_opt'].values)**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect on spark and stop time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_optires_pso_rd = pd.read_pickle('optires_pso.pkl')\n",
    "df_optires_pso_rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prikkel and stop for every optimisation result\n",
    "n_opt_tot = len(df_optires_pso_rd)\n",
    "sparklist, stoplist = np.zeros((n_opt_tot,1)), np.zeros((n_opt_tot,1))\n",
    "for r in range(n_opt_tot):\n",
    "    try:\n",
    "        spark, stop = spark_stop(df_optires_pso_rd.iloc[r,:4])\n",
    "    except:\n",
    "        spark, stop = np.nan, np.nan\n",
    "    sparklist[r] = spark\n",
    "    stoplist[r] = stop\n",
    "\n",
    "df_optires_pso_rd.loc[:,'prikkel'] = sparklist  \n",
    "df_optires_pso_rd.loc[:,'stop'] = stoplist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optires_pso_rd.to_excel('opti_results_pso_20200430.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wishlist\n",
    "\n",
    "* model that relates parameters to optimal training time - model sensitivity\n",
    "* validation: \n",
    "    * apply average model (average of fitted parameters?) to all datasets\n",
    "    * leave one out (opletten welke punten)\n",
    "* measurement method: check results global optimisation\n",
    "    * consistency over sporters (tau should be similar for sporters)\n",
    "    \n",
    "    \n",
    "**To do**\n",
    "* refine grid: request however not possible, lowering one tenth... otherwise 324 trillion simulations\n",
    "* Banister model global output for each method + effect on stop and spark time\n",
    "    * global optimisation each method\n",
    "    * sensitivity analysis on stop/spark time outcome\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(<a id=\"cit-banister1975systems\" href=\"#call-banister1975systems\">Banister, Calvert <em>et al.</em>, 1975</a>) Banister E W, Calvert T W, Savage M V <em>et al.</em>, ``_A systems model of training for athletic performance_'', Aust J Sport Med, vol. 7, number 3, pp. 57--61,  1975.\n",
    "\n",
    "(<a id=\"cit-busso1994\" href=\"#call-busso1994\">Busso, Candau <em>et al.</em>, 1994</a>) Busso Thierry, Candau Robin and Lacour Jean Ren{\\'{e}}, ``_Fatigue and fitness modelled from the effects of training on performance_'', Eur J Appl Physiol Occup Physiol, vol. 69, number 1, pp. 50--54, jan 1994.  [online](https://www.researchgate.net/publication/15242395)\n",
    "\n",
    "(<a id=\"cit-Hellard2006\" href=\"#call-Hellard2006\">Hellard, Avalos <em>et al.</em>, 2006</a>) Hellard Philippe, Avalos Marta, Lacoste Lucien <em>et al.</em>, ``_Assessing the limitations of the Banister model in monitoring training_'', J Sports Sci, vol. 24, number 5, pp. 509--520,  2006.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "275px",
    "width": "300px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
